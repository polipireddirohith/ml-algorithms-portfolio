# Decision Trees - Iris Classification

## ğŸ“– Overview

Decision Trees are versatile supervised learning algorithms that can perform both classification and regression. This project demonstrates multi-class classification on the famous Iris dataset.

## ğŸ¯ Algorithm Explanation

**Decision Trees** create a tree-like model of decisions based on feature values:

- **Root Node**: Entire dataset
- **Internal Nodes**: Decision based on features
- **Leaf Nodes**: Final prediction
- **Branches**: Outcome of a decision

## ğŸ”‘ Key Concepts

1. **Splitting Criteria**: Gini Impurity, Entropy, Information Gain
2. **Tree Depth**: Controls model complexity
3. **Pruning**: Prevents overfitting
4. **Feature Importance**: Based on split quality

## ğŸ“Š Dataset

- **Features**: Sepal length, sepal width, petal length, petal width
- **Target**: Iris species (Setosa, Versicolor, Virginica)
- **Size**: 150 samples (classic dataset)

## ğŸ¨ Visualizations

- Decision tree structure
- Feature importance
- Confusion matrix
- Decision boundaries

## ğŸ“ˆ Performance Metrics

- Accuracy
- Precision, Recall, F1-Score per class
- Confusion Matrix
- Classification Report

## ğŸš€ Usage

```bash
pip install -r requirements.txt
python main.py
```

## ğŸ’¡ Interview Questions Covered

1. How do Decision Trees work?
2. What is Gini Impurity vs Entropy?
3. How to prevent overfitting in Decision Trees?
4. What is pruning?
5. Advantages and disadvantages of Decision Trees?

## ğŸ” Real-World Applications

- Medical diagnosis
- Credit approval
- Customer segmentation
- Fraud detection
